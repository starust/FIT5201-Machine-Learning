{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 1.2: Bootstrap\n",
    "Last modified (13 Feb 2023)\n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "In this activity we learn how to:\n",
    "\n",
    "- define a repeated data splitting (cross validation) scheme in `scikit-learn`\n",
    "- implement the Bootstrap in the splitter framework (using `numpy`)\n",
    "- apply the Bootstrap to more reliably estimate the effect of a hyper-parameter (using [`sklearn.model_selection.cross_validate`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate))\n",
    "- plot the results of an experiment with error bars using `matplotlib`\n",
    "- choose a suitable number of Bootstrap repetitions\n",
    "- describe how the number of training data influences the choice for $k$ in $k$-NN\n",
    "\n",
    "### Preliminaries\n",
    "\n",
    "- Activity 1.1\n",
    "- Lecture 2\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous activity we have determined that drawing conclusions about predictive performance from a single train/test split is risky, because our error estimates will often have a large variance. Here, the source of randomness is the specific split but also the overall data sample we got in the first place (imagine Fisher would have obtained a second sample of Iris flowers; would it have been identical to the first one?). \n",
    "\n",
    "Therefore, we now look at more systematic to obtain error estimates that take this variation over possible training sets into account. In Chapter 3, we have learned that Bootstrapping is a powerful statistical tool that helps us to measure the uncertainty in the prediction of a model. In this activity, we implement this technique to assess variations in the prediction of KNN classifier. In **Assignment Task 2.B.**, you will be asked to expand this example and develop a bootstrap procedure for KNN as a regressor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "\n",
    "- Before turning to the Bootstrap, we will first learn how data splitting and subsampling strategies are generally described and implemented in `scikit-learn`. \n",
    "- We will demonstrate that using again the Iris dataset and the $k$-NN classifier. Instead of re-implementing $k$-NN, this time we will simply use the implementation from `scikit-learn`.\n",
    "- Then we will implement a Bootstrap splitting scheme and evaluate the effect of the number of repetitions and training size in determining what is the best value for $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitters in `scikit-learn`\n",
    "\n",
    "In the framework of `scikit-learn`, the Boostrap can be considered a particular cross validation scheme where we repeatedly generate training sets via index sampling with replacement and use the unsampled indices in a given iteration as test indices. \n",
    "\n",
    "Such schemes are implemented in `skikit-learn` as `Splitter` objects that provide two methods:\n",
    "\n",
    "- `get_n_splits(x=None, y=None, groups=None)` which informs about how many splits the splitter would produce for the given dataset; here all parameters are optional as the splits (often) do not depend on the given number\n",
    "- `split(x, y=None, groups=None)` which is a [generator function](https://docs.python.org/3/reference/expressions.html?highlight=yield%20statement#yield-expressions) that yields a sequence of training and test indices\n",
    "\n",
    "As an illustration let us look at the class `ShuffleSplit` and use it with the Iris dataset. This splitting scheme allows as to perform a simple training/test split akin to what we have implemented in Activity 1.1. However, it conveniently allows for repetitions of the splitting process to increase our confidence in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle split will generate 2 splits.\n",
      "split 0\n",
      "train indices [ 85  30 101  94  64  89  91 125  48  13 111  95  20  15  52   3 149  98\n",
      "   6  68 109  96  12 102 120 104 128  46  11 110 124  41 148   1 113 139\n",
      "  42   4 129  17  38   5  53 143 105   0  34  28  55  75  35  23  74  31\n",
      " 118  57 131  65  32 138  14 122  19  29 130  49 136  99  82  79 115 145\n",
      "  72  77  25  81 140 142  39  58  88  70  87  36  21   9 103  67 117  47]\n",
      "test indices [114  62  33 107   7 100  40  86  76  71 134  51  73  54  63  37  78  90\n",
      "  45  16 121  66  24   8 126  22  44  97  93  26 137  84  27 127 132  59\n",
      "  18  83  61  92 112   2 141  43  10  60 116 144 119 108  69 135  56  80\n",
      " 123 133 106 146  50 147]\n",
      "split 1\n",
      "train indices [  7  45 129 103 146 120  94  50 134  99 126 114   9  39  97 101  29  81\n",
      "  20  46  51  53  23  27   2  28  37 111  10  84 137 127  43  87  69 144\n",
      " 140  35  76   3  82 145 116  88  44 147   1  93  38  11 115  54  40  18\n",
      "  41  79  24  56  71  13  31  85  70 132 125 123 100  32 104  83 117 118\n",
      " 138  25 110  16  75 109 121  86 139   4  96  14  61  67 149  95  19  72]\n",
      "test indices [ 92 141 130 119  48 143 122  63  26  64  42 108  91  77  22 148   6  65\n",
      "  47  68  60  15 124  58 142  12  59 105  89  78  52 131 113  98  30 136\n",
      "  66 133  49  62  74  17 106   8 135  80 107  90   0  36 112   5  57 102\n",
      "  55  34 128  33  21  73]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "iris = load_iris()\n",
    "shuffleSplit = ShuffleSplit(n_splits=2, train_size=0.6, random_state=0)\n",
    "print('Shuffle split will generate', shuffleSplit.get_n_splits(), 'splits.')\n",
    "for i, (train_idx, test_idx) in enumerate(shuffleSplit.split(iris.data)):\n",
    "    print('split', i)\n",
    "    print('train indices', train_idx)\n",
    "    print('test indices', test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we could use such a splitter in an explicit loop as above to run an experiment, it is often more convenient to use the function [`sklearn.model_selection.cross_valide`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate). Let us demonstrate this function with 5 repetitions of shuffle splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00200057, 0.00100279, 0.        , 0.00100088, 0.0009985 ]),\n",
       " 'score_time': array([0.00400901, 0.00251198, 0.00330448, 0.00352287, 0.00629425]),\n",
       " 'test_score': array([0.93333333, 0.93333333, 0.98333333, 0.93333333, 0.96666667]),\n",
       " 'train_score': array([0.98888889, 0.96666667, 0.94444444, 0.96666667, 0.95555556])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(3)\n",
    "shuffleSplit5 = ShuffleSplit(n_splits=5, train_size=0.6, random_state=0)\n",
    "cross_validate(knn, iris.data, iris.target, cv=shuffleSplit5, return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `cross_validate` returns a dictionary that not only contains the test and train prediction metrics for all repetitions defined by the splitter but also the time that was needed for fitting and prediction (which will be useful when aiming to optimise an implementation.\n",
    "\n",
    "One thing that is noteworthy is that the metric used here is not the error rate but the the inverse accuracy metric, i.e., one minus the error rate. Below we show how we can provide our own metric based on providing a per-data-point error (or loss) function. Here, we use the zero-one error, which results in our familiar error rate as score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00100017, 0.        , 0.        , 0.00100875, 0.        ]),\n",
       " 'score_time': array([0.00424218, 0.00353384, 0.00454164, 0.0029912 , 0.00299811]),\n",
       " 'test_score': array([0.06666667, 0.06666667, 0.01666667, 0.06666667, 0.03333333]),\n",
       " 'train_score': array([0.01111111, 0.03333333, 0.05555556, 0.03333333, 0.04444444])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer, zero_one_loss\n",
    "\n",
    "cross_validate(knn, iris.data, iris.target, cv=shuffleSplit5, return_train_score=True, scoring=make_scorer(zero_one_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Bootstrap\n",
    "\n",
    "#### Task A: Complete the Implementation of the bootstrap splitter\n",
    "\n",
    "**Insert the line in the Bootstrap splitter implementation below that generates the train indices in a given iteration**\n",
    "\n",
    "Hint: You can go back to Activity 1.1 to remind yourself how we have performed the single train/test split. What parameter has to change to sample the training indices for the Bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3186803333.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 15\u001b[1;36m\u001b[0m\n\u001b[1;33m    train_idx = # YOUR CODE HERE\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BootstrapSplitter:\n",
    "\n",
    "    def __init__(self, reps, train_size, random_state=None):\n",
    "        self.reps = reps\n",
    "        self.train_size = train_size\n",
    "        self.RNG = np.random.default_rng(random_state)\n",
    "\n",
    "    def get_n_splits(self):\n",
    "        return self.reps\n",
    "\n",
    "    def split(self, x, y=None, groups=None):\n",
    "        for _ in range(self.reps):\n",
    "            train_idx = # YOUR CODE HERE\n",
    "            test_idx = np.setdiff1d(np.arange(len(x)), train_idx)\n",
    "            np.random.shuffle(test_idx)\n",
    "            yield train_idx, test_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap = BootstrapSplitter(5, 0.6, random_state=0)\n",
    "cross_validate(knn, iris.data, iris.target, cv=bootstrap, return_train_score=True, scoring=make_scorer(zero_one_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to Test the Effect of $k$ in $k$-NN\n",
    "\n",
    "The following function allows us to run a series of bootstrap experiments, one for each value of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(max_k, cv):\n",
    "    r = cv.get_n_splits()\n",
    "    test_results = np.zeros(shape=(r, max_k))\n",
    "    train_results = np.zeros(shape=(r, max_k))\n",
    "    for k in range(1, max_k+1):\n",
    "        knn = KNeighborsClassifier(k)\n",
    "        cv_res = cross_validate(knn, iris.data, iris.target, cv=cv, return_train_score=True, scoring=make_scorer(zero_one_loss))\n",
    "        test_results[:, k-1] = cv_res['test_score']\n",
    "        train_results[:, k-1] = cv_res['train_score']\n",
    "\n",
    "    return train_results, test_results\n",
    "\n",
    "reps=25\n",
    "max_k=30\n",
    "train_results, test_results = evaluation(max_k=max_k, cv=BootstrapSplitter(reps, 0.6, random_state=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the Results\n",
    "\n",
    "When visualising the results of the experiments we now can compute meaningful error bars for each estimated performance due to the performed repetitions. This can be done with the `matplotlib`-function `errorbar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "ks = np.arange(1, max_k+1)\n",
    "_, axs = plt.subplots(1, 2, figsize=(8,4), tight_layout=True, sharey=True)\n",
    "z = (reps**0.5)/1.96\n",
    "axs[0].errorbar(ks, train_results.mean(axis=0), yerr=train_results.std(axis=0)/z, marker='o', label='train')\n",
    "axs[0].errorbar(ks, test_results.mean(axis=0), yerr=test_results.std(axis=0)/z, marker='o', label='test')\n",
    "axs[0].legend()\n",
    "axs[0].set_xlabel('$k$')\n",
    "axs[0].set_ylabel('error rate')\n",
    "axs[1].errorbar(1/ks, train_results.mean(axis=0), yerr=train_results.std(axis=0)/z, marker='o', label='train')\n",
    "axs[1].errorbar(1/ks, test_results.mean(axis=0), yerr=test_results.std(axis=0)/z, marker='o', label='test')\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_xlabel('$1/k$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative visualisation that uses [boxplots](https://en.wikipedia.org/wiki/Box_plot), which can be plotted with the function [`matplotlib.pyplot.boxplot`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.boxplot.html). Since there are more details about the result distribution in a boxplot it makes sense to focus on one individual metric in such a plot (here we use the test error rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(test_results, showfliers=False)\n",
    "plt.ylabel('test error rate')\n",
    "plt.xlabel('$k$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Bootstrap Repetitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task B: Give a hypothesis and rationale how changing the number of bootstrap repetitions will change the results of the experiment\n",
    "\n",
    "*[YOUR ANSWER HERE]*\n",
    "\n",
    "#### Task C: Test your hypothesis by completing the experiment code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitions = [25, 50, 100]\n",
    "train_results_reps = []\n",
    "test_results_reps = []\n",
    "\n",
    "for r in repetitions:\n",
    "    print('reps: ', r)\n",
    "    train_res, test_res = # YOUR CODE HERE\n",
    "    train_results_reps.append(train_res)\n",
    "    test_results_reps.append(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.arange(1, max_k+1)\n",
    "_, axs = plt.subplots(1, len(repetitions), figsize=(4*len(repetitions),4), tight_layout=True, sharey=True)\n",
    "for i, r in enumerate(repetitions):\n",
    "    z = r**0.5/1.96\n",
    "    axs[i].set_title(f'$r$={r}')\n",
    "    axs[i].errorbar(ks, train_results_reps[i].mean(axis=0), yerr=train_results_reps[i].std(axis=0)/z, marker='o', label='train')\n",
    "    axs[i].errorbar(ks, test_results_reps[i].mean(axis=0), yerr=test_results_reps[i].std(axis=0)/z, marker='o', label='test')\n",
    "    axs[i].set_xlabel('$k$')\n",
    "axs[0].legend()    \n",
    "axs[0].set_ylabel('error rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Train Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task D: Give a hypothesis and rationale how changing the train size will change the results of the experiment and in particular how it will influence what $k$ will achieve the best test performance.\n",
    "\n",
    "*[YOUR ANSWER HERE]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sizes = [0.4, 0.6, 0.8]\n",
    "train_results_trainsize = []\n",
    "test_results_trainsize = []\n",
    "reps = 100\n",
    "\n",
    "for frac in training_sizes:\n",
    "    print('train size: ', round(len(iris.data)*frac))\n",
    "    train_res, test_res = evaluation(max_k, BootstrapSplitter(reps, frac, random_state=1))\n",
    "    train_results_trainsize.append(train_res)\n",
    "    test_results_trainsize.append(test_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Determine the best $k$ per train size.\n",
    "\n",
    "**Add code below to compile the list of optimal $k$ values corresponding to the train sizes.**\n",
    "\n",
    "Hint: Check the documentation of [`np.argmin`](https://numpy.org/doc/stable/reference/generated/numpy.argmin.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ks = # YOUR CODE HERE\n",
    "best_ks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot all results including your determined optimal $k$ values to check your hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.arange(1, max_k+1)\n",
    "_, axs = plt.subplots(1, len(training_sizes), figsize=(4*len(training_sizes),4), tight_layout=True, sharey=True)\n",
    "for i, frac in enumerate(training_sizes):\n",
    "    z = reps**0.5/1.96\n",
    "    axs[i].set_title(f'$n$={round(len(iris.data)*frac)}')\n",
    "    axs[i].errorbar(ks, train_results_trainsize[i].mean(axis=0), yerr=train_results_trainsize[i].std(axis=0)/z, marker='o', label='train')\n",
    "    axs[i].errorbar(ks, test_results_trainsize[i].mean(axis=0), yerr=test_results_trainsize[i].std(axis=0)/z, marker='o', label='test')\n",
    "    axs[i].axvline(ks[best_ks[i]], color='black', linestyle='--', label='best $k$')\n",
    "    axs[i].set_xlabel('$k$')\n",
    "axs[0].legend()    \n",
    "axs[0].set_ylabel('error rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "#### Task F: Summarise what you have learned by briefly answering the following questions.\n",
    "\n",
    "**What is the purpose of the Bootstrap when evaluating machine learning algorithms and how should you choose the number of Bootstrap repetitions?**\n",
    "\n",
    "*[YOUR ANSWER HERE]*\n",
    "\n",
    "**How does the number of available training data points influence the choice of the hyper-parameter $k$ in $k$-NN, and what is the reason for this?**\n",
    "\n",
    "*[YOUR ANSWER HERE]*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
